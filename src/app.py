"""Main application module for the AI assistant."""
import logging
from pathlib import Path
import os
import time
import torch
import json
import re
from typing import Optional, Dict, Any, List, Union

import numpy as np
import sounddevice as sd
import soundfile as sf
from loguru import logger

from .config import config
from .memory.memory_manager import MemoryManager
from .memory.memory_utils import get_memory_statistics
from .memory.models import Memory, MemoryType, create_session_factory
from .stt import WhisperSTT

class AIAssistant:
    """Main AI Assistant class that ties together all components."""
    
    def __init__(self):
        """Initialize the AI Assistant."""
        self._setup_logging()
        self.llm_params = {}
        self.memory_manager = self._init_memory()
        self.llm = self._init_llm()
        self.stt = self._init_speech_to_text()
        self._recording = False
        self._audio_buffer = []
        self._sample_rate = config["audio.sample_rate"]
        self._channels = config["audio.channels"]
        self._silence_duration = config["audio.silence_duration"]
        self._vad_aggressiveness = config["audio.vad_aggressiveness"]
        self._input_device = config["audio.input_device"]
        
        if self.llm is None:
            logger.warning("LLM initialization failed. Some features may not work correctly.")
        
    def _setup_logging(self) -> None:
        """Configure logging for the application."""
        log_level = config["logging.level"]
        log_file = Path(config["logging.file"])
        
        # Ensure log directory exists
        log_file.parent.mkdir(parents=True, exist_ok=True)
        
        # Configure loguru
        logger.remove()  # Remove default handler
        logger.add(
            log_file,
            level=log_level,
            rotation=config["logging.rotation"],
            retention=config["logging.retention"],
            enqueue=True,
            backtrace=True,
            diagnose=config["app.debug"]
        )
        
        # Also log to console in debug mode
        if config["app.debug"]:
            logger.add(
                lambda msg: print(msg, end=""),
                level=log_level,
                colorize=True
            )
    
    def _init_memory(self) -> MemoryManager:
        """Initialize the memory system."""
        logger.info("Initializing memory system...")
        
        # Ensure the database is properly initialized with the latest schema
        from sqlalchemy import create_engine
        from .memory.models import init_db
        
        engine = create_engine(config["memory.db_url"])
        init_db(engine)
        
        return MemoryManager(
            db_url=config["memory.db_url"],
            faiss_index_path=config["memory.faiss_index_path"],
            embedding_dim=config["memory.embedding_dim"]
        )
    
    def _init_llm(self) -> Any:
        """Initialize the language model using llama-cpp-python.
        
        Returns:
            An instance of the Llama language model or None if initialization fails
        """
        try:
            from llama_cpp import Llama
            import os
            from pathlib import Path
            
            # Get LLM configuration with defaults
            llm_config = config.get("llm", {})
            model_path = llm_config.get("model_path")
            
            logger.info(f"Initializing LLM with config: {llm_config}")
            
            if not model_path:
                # Try to find the model in the models directory
                models_dir = Path("models/llm")
                logger.info(f"Looking for model in: {models_dir.absolute()}")
                if models_dir.exists():
                    model_files = list(models_dir.glob("*.gguf"))
                    if model_files:
                        model_path = str(model_files[0])
                        logger.info(f"Using model: {model_path}")
                    else:
                        logger.error("No .gguf model files found in models/llm/")
                        return None
                else:
                    logger.error("models/llm directory not found")
                    return None
            
            # Resolve the model path
            model_path = Path(model_path)
            if not model_path.is_absolute():
                # If path is relative, make it relative to the project root
                project_root = Path(__file__).parent.parent
                model_path = project_root / model_path
            
            model_path = str(model_path.absolute())
            
            # Check if model file exists
            if not os.path.exists(model_path):
                logger.error(f"LLM model file not found at {model_path}")
                return None
            
            logger.info(f"Loading LLM model from {model_path}")
            
            # Get LLM configuration with defaults and ensure proper types
            context_window = int(llm_config.get("context_window", 2048))
            n_threads = int(os.cpu_count() or 4)
            debug_mode = bool(config.get("app", {}).get("debug", False))
            
            logger.info(f"Initializing LLM with context_window={context_window}, n_threads={n_threads}")
            
            # Initialize the LLM with all required parameters
            llm = Llama(
                model_path=model_path,
                n_ctx=context_window,
                n_threads=n_threads,
                n_gpu_layers=0,  # Set to >0 to use GPU acceleration if available
                verbose=debug_mode
            )
            
            # Store LLM parameters for later use
            self.llm_params = {
                "max_tokens": int(llm_config.get("max_tokens", 200)),
                "temperature": float(llm_config.get("temperature", 0.7)),
                "top_p": float(llm_config.get("top_p", 0.9)),
                "top_k": int(llm_config.get("top_k", 40)),
                "repeat_penalty": float(llm_config.get("repeat_penalty", 1.1))
            }
            
            # Test the model
            try:
                test_output = llm.create_chat_completion(
                    messages=[{"role": "user", "content": "Hello!"}],
                    max_tokens=10,
                    temperature=0.0
                )
                logger.debug(f"LLM test response: {test_output}")
                return llm
                
            except Exception as e:
                logger.error(f"LLM test generation failed: {str(e)}")
                return None
            
        except ImportError as e:
            logger.error("Failed to import required modules. Make sure llama-cpp-python is installed.")
            logger.error(f"Error: {e}")
            return None
        except Exception as e:
            logger.error(f"Failed to initialize LLM: {e}", exc_info=True)
            return None
    
    def _init_speech_to_text(self) -> WhisperSTT:
        """Initialize the speech-to-text system.
        
        Returns:
            Initialized WhisperSTT instance
        """
        try:
            import torch
            return WhisperSTT(
                model_size=config["stt.model_size"],
                device="cuda" if torch.cuda.is_available() else "cpu"
            )
        except ImportError:
            logger.warning("PyTorch not available, using CPU for Whisper")
            return WhisperSTT(
                model_size=config["stt.model_size"],
                device="cpu"
            )
        except Exception as e:
            logger.error(f"Failed to initialize speech-to-text: {e}")
            raise
    
    def _build_context(self, memories: List[Union[Memory, Dict[str, Any]]]) -> str:
        """Build a context string from a list of relevant memories.
        
        Args:
            memories: List of Memory objects or dictionaries
            
        Returns:
            Formatted context string
        """
        if not memories:
            return "No relevant context found."
        
        context_parts = ["Relevant context from memory:"]
        
        for i, memory in enumerate(memories, 1):
            # Handle both Memory objects and dictionaries
            if hasattr(memory, 'content'):  # It's a Memory object
                memory_content = memory.content.strip()
                memory_type = memory.memory_type.value.capitalize()
                memory_time = memory.created_at.strftime("%Y-%m-%d %H:%M")
                source = f" (Source: {memory.source})" if memory.source else ""
            else:  # It's a dictionary
                memory_content = str(memory.get('content', '')).strip()
                memory_type = memory.get('memory_type', 'unknown').capitalize()
                created_at = memory.get('created_at')
                memory_time = created_at.strftime("%Y-%m-%d %H:%M") if hasattr(created_at, 'strftime') else 'unknown'
                source = f" (Source: {memory.get('source')})" if memory.get('source') else ""
            
            # Add to context
            context_parts.append(
                f"{i}. [{memory_type}{source}, {memory_time}]: {memory_content}"
            )
        
        return "\n".join(context_parts)
    
    def _generate_response(self, query: str, context: str) -> str:
        """Generate a response using the LLM.
        
        Args:
            query: The user's query
            context: Context from memory
            
        Returns:
            Generated response
        """
        try:
            logger.info(f"Generating response for query: {query}")
            
            if not hasattr(self, 'llm') or self.llm is None:
                error_msg = "LLM not initialized"
                logger.error(error_msg)
                raise ValueError(error_msg)
            
            if not hasattr(self, 'llm_params'):
                error_msg = "LLM parameters not initialized"
                logger.error(error_msg)
                raise ValueError(error_msg)
            
            # Prepare the prompt with system message and context
            system_message = (
                "You are a helpful AI assistant. Use the following context to answer the question. "
                "If you don't know the answer, say you don't know. Be concise and accurate."
            )
            
            messages = [
                {"role": "system", "content": system_message},
                {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {query}"}
            ]
            
            logger.debug(f"Sending to LLM: {json.dumps(messages, indent=2)}")
            
            # Ensure all parameters are valid
            llm_params = {
                'messages': messages,
                'max_tokens': int(self.llm_params.get("max_tokens", 200)),
                'temperature': float(self.llm_params.get("temperature", 0.7)),
                'top_p': float(self.llm_params.get("top_p", 0.9)),
                'top_k': int(self.llm_params.get("top_k", 40)),
                'repeat_penalty': float(self.llm_params.get("repeat_penalty", 1.1)),
                'stop': ["\n###", "\nUser:", "\n###", "\n\n"]
            }
            
            logger.debug(f"LLM parameters: {json.dumps(llm_params, indent=2, default=str)}")
            
            # Generate response using the LLM
            try:
                response = self.llm.create_chat_completion(**llm_params)
                logger.debug(f"Raw LLM response: {json.dumps(response, indent=2, default=str)}")
                
                # Extract the response text
                if not response or 'choices' not in response or not response['choices']:
                    error_msg = f"Invalid response format from LLM: {response}"
                    logger.error(error_msg)
                    raise ValueError(error_msg)
                
                response_text = response['choices'][0]['message']['content'].strip()
                
                if not response_text:
                    error_msg = "Empty response from LLM"
                    logger.error(error_msg)
                    raise ValueError(error_msg)
                
                # Clean the response
                def clean_response(text: str) -> str:
                    """Clean and format the response text from the LLM."""
                    if not text:
                        return text
                    
                    try:
                        # Common patterns to remove or replace
                        patterns = [
                            # Remove [INST]...[/INST] and similar tags
                            (r'\[(?:INST|SYS|/INST|/SYS)\][\s\S]*?(?=\[(?:/)?(?:INST|SYS)\]|$)', ''),
                            # Remove any remaining tags in square brackets
                            (r'\[[^\]]*\]', ''),
                            # Remove common prefixes/suffixes
                            (r'^(?:AI:|Assistant:|Q:|A:|\d+\.?\s*|[-*â€¢]\s*)', '', re.IGNORECASE),
                            # Normalize whitespace and clean up
                            (r'\s+', ' '),
                            (r'\s*[\r\n]+\s*', '\n'),
                            (r'[\x00-\x1F\x7F]', ' '),  # Remove control characters
                        ]
                        
                        # Apply all patterns
                        cleaned = text
                        for pattern in patterns:
                            if isinstance(pattern, tuple) and len(pattern) == 3:
                                cleaned = re.sub(pattern[0], pattern[1], cleaned, flags=pattern[2])
                            elif isinstance(pattern, tuple) and len(pattern) == 2:
                                cleaned = re.sub(pattern[0], pattern[1], cleaned, flags=re.IGNORECASE | re.MULTILINE)
                            else:
                                cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE | re.MULTILINE)
                        
                        # Final cleanup
                        cleaned = (
                            cleaned.strip()
                            .replace('  ', ' ')
                            .replace(' .', '.')
                            .replace(' ,', ',')
                            .replace(' ?', '?')
                            .replace(' !', '!')
                        )
                        
                        # Capitalize first letter and ensure it ends with punctuation
                        if cleaned:
                            cleaned = cleaned[0].upper() + cleaned[1:]
                            if cleaned[-1] not in '.!?':
                                cleaned += '.'
                        
                        return cleaned.strip()
                        
                    except Exception as e:
                        logger.error(f"Error in clean_response: {str(e)}", exc_info=True)
                        return text.strip()
                
                clean_text = clean_response(response_text)
                logger.info(f"Generated response: {clean_text}")
                return clean_text
                
            except Exception as e:
                error_msg = f"Error generating LLM response: {str(e)}"
                logger.error(error_msg, exc_info=True)
                return f"I'm sorry, I encountered an error while generating a response: {str(e)}"
            
        except Exception as e:
            logger.error(f"Error generating response: {str(e)}", exc_info=True)
            return f"I'm sorry, I encountered an error while processing your request: {str(e)}"
    
    def process_query(self, query: Union[str, np.ndarray], user_id: str = "default") -> Dict[str, Any]:
        """Process a user query, which can be either text or audio data.
        
        Args:
            query: The user's query as text or audio data
            user_id: ID of the user making the query
            
        Returns:
            Dictionary containing the response and any metadata
        """
        try:
            # Handle audio input
            if isinstance(query, np.ndarray):
                query = self._transcribe_audio(query)
                if not query.strip():
                    return {"response": "I couldn't understand your audio. Please try again."}
            
            # Get relevant context from memory
            context = self._get_context(query, user_id)
            
            # Check if LLM is available
            if self.llm is None:
                # Fallback response if LLM is not available
                if "capital" in query.lower() and ("france" in query.lower() or "paris" in query.lower()):
                    return {"response": "The capital of France is Paris."}
                elif "train" in query.lower() and "mph" in query.lower() and "hours" in query.lower():
                    return {"response": "If a train travels 60 mph for 2.5 hours, it will travel 150 miles."}
                else:
                    return {"response": "I'm currently experiencing technical difficulties. Please try again later."}
            
            # Generate response using LLM with context
            try:
                # Prepare the prompt
                messages = [
                    {"role": "system", "content": "You are a helpful AI assistant."},
                    {"role": "user", "content": f"Context: {context}\n\nQuestion: {query}"}
                ]
                
                # Generate response
                response = self.llm.create_chat_completion(
                    messages=messages,
                    max_tokens=self.llm_params.get("max_tokens", 200),
                    temperature=self.llm_params.get("temperature", 0.7),
                    top_p=self.llm_params.get("top_p", 0.9),
                    top_k=self.llm_params.get("top_k", 40),
                    repeat_penalty=self.llm_params.get("repeat_penalty", 1.1)
                )
                
                # Extract the response text
                response_text = response['choices'][0]['message']['content'].strip()
                
            except Exception as e:
                logger.error(f"Error generating LLM response: {str(e)}", exc_info=True)
                response_text = "I'm sorry, I encountered an error while generating a response."
            
            # Store the interaction in memory if it's a meaningful exchange
            if query.strip() and len(query) > 3:  # Don't store very short queries
                try:
                    self.memory_manager.store_memory(
                        query=query,
                        response=response_text,
                        user_id=user_id,
                        context=context
                    )
                except Exception as e:
                    logger.error(f"Error storing memory: {str(e)}", exc_info=True)
            
            return {
                "response": response_text,
                "context": context if config.get('app.debug') else None
            }
            
        except Exception as e:
            logger.error(f"Error processing query: {str(e)}", exc_info=True)
            return {"response": "I encountered an error processing your request. Please try again."}
    
    def _get_context(self, query: str, user_id: str) -> str:
        """Retrieve relevant context from memory for the given query."""
        try:
            # Search for relevant memories
            memories = self.memory_manager.search_memories(
                query=query,
                user_id=user_id,
                limit=3
            )
            
            if not memories:
                return ""
                
            # Format memories into context
            context = "Relevant context from previous conversations:\n"
            for i, memory in enumerate(memories, 1):
                # Handle both Memory objects and dictionaries
                if hasattr(memory, 'content'):
                    content = memory.content
                else:
                    content = memory.get('content', '')
                context += f"{i}. {content}\n"
            
            return context
            
        except Exception as e:
            logger.error(f"Error retrieving context: {str(e)}", exc_info=True)
            return ""

    def _transcribe_audio(self, audio_data: np.ndarray) -> str:
        """Transcribe audio data to text."""
        try:
            # Ensure audio data is writable (fixes PyTorch warning)
            if not audio_data.flags.writeable:
                audio_data = audio_data.copy()
                
            # Normalize audio if needed
            if audio_data.dtype != np.int16:
                audio_data = (audio_data * 32767).astype(np.int16)
                
            return self.stt.transcribe(audio_data)
            
        except Exception as e:
            logger.error(f"Audio transcription failed: {str(e)}", exc_info=True)
            return ""
